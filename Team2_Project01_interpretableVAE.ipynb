{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5Ow8XOaO1mL"
      },
      "source": [
        "# Project: an interpretable VAE to study Transcription Factor rewiring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6QqHzxdPIHG"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Contact: Carl Herrmann, Daria Doncevic (Heidelberg University carl.herrmann@bioquant.uni-heidelberg.de daria.doncevic@bioquant.uni-heidelberg.de)\n",
        "\n",
        "Variational Autoencoders are now widely applied to capture the complexity in single-cell omics datasets and can be used to replace complete workflows of data analysis consisting of normalization, clustering, celltype annotation and differential expression analysis. However, due to their non-linear nature, they lack interpretability. As you have learned in week 2 of the course, a subset of VAE models such as the VEGA model ([Seninge *et al* (2021)](https://www.nature.com/articles/s41467-021-26017-0)) can convey intrinsic interpretability by incorporating biological networks directly into their model structure. In VEGA, every node in the latent space corresponds to a biological entity such as a pathway or transcription factor (TF) and is then connected only to the genes in the reconstruction layer that are annotated to that biological entity, making the one-layered decoder sparse. Thus, the activations of the nodes in the latent space can be interpreted as pathway or TF activities. In this project, you are going to focus more on the weights of the decoder connections than on the latent space activities, and see how they differ for different genes for a given TF. We believe that this change in the weights between TF and target genes might indicate a rewiring of the TF in different biological condition.\n",
        "You are also going to estimate uncertainties for the weights (which also depends on the annotation that is used) and to investigate how the weights change in condition versus control, which we call \"Transcription factor rewiring\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZXteiFEIPl4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UMghjAISX3W"
      },
      "source": [
        "## Goal\n",
        "The goal of this project is to study Transcription factor rewiring in the context of interferon response. Thus, you should focus especially on TFs that are known to play a role in interferon response, such as STAT1, STAT2, and the different IRFs, and their target genes. You can read more about that [in this review](https://www.nature.com/articles/nri3787). To be able to obtain different weights for condition and control, you will have to train two separate models. Since VAEs are probabilistic, you will end up with a different set of weights each time you train the model. Thus, your task will also be to estimate or compute uncertainties for the weights. There are different ways to achieve this:\n",
        "\n",
        "\n",
        "*   Train each model multiple times (and then compute metrics such as mean and standard deviation for the different weights). Are there genes whose annotation to a given TF seems to be \"less certain\" than for others?\n",
        "*   When we train multiple models and then aggregate their results, we also call this an ensemble. Training an ensemble is often expensive in terms of computational time and power. Stochastic Weight Averaging (SWA) has been proposed as a method that can yield more robust weights without the need of training ensembles. The publication is linked [here](https://arxiv.org/pdf/1803.05407), and additional information can be found [here](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/). Try to implement this approach for your VEGA model.\n",
        "*   Bayesian Neural Networks do built-in estimation of weight uncertainties. Try to replace your linear layer with a Bayesian layer to obtain these estimates. You can find some information to get you started [in this blogpost](https://towardsdatascience.com/from-theory-to-practice-with-bayesian-neural-network-using-python-9262b611b825/). More mathematical foundations can also be found [here](https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvq2T8vkhYPe"
      },
      "source": [
        "Do not forget to also include some biological speculation/interpretation of your results!\n",
        "\n",
        "Questions\n",
        "\n",
        "1) Which data we should choose for selecting weights -> processed data or raw regulon data -> if raw regulon data which column we should choose such as stimulation or inhibition colummn\n",
        "\n",
        "2) In the mask, does 1 mean TF is regulating the gene and 0 means there is no regulation? What happens if we put -1 in the mask, as a way of incorporating inhibition regulation?\n",
        "\n",
        "3) How to organize the final result? presentation? poster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxFPB6FOSiyW"
      },
      "source": [
        "## Data and model\n",
        "\n",
        "As in week 2, you will use a dataset that contains peripheral blood mononuclear cells (PBMCs) from systemic lupus patients, treated with Interferon beta or untreated (control). You can follow the steps from week 2's notebook to download the data. You will also work with a VEGA-like model, and you can reuse code from week 2 for the implementation.\n",
        "\n",
        "This time, instead of Reactome pathways, you will use TF regulons from the collecTRI database as a biological prior. Information about this resource and how to obtain the data is available [here](https://github.com/saezlab/CollecTRI). You might want to filter the regulons based on regulon size prior to incorporation into the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2MHQc_DO50h"
      },
      "source": [
        "# Load packages and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7fjLZZRxPEY_",
        "outputId": "efbd19a7-54ab-4661-a5d3-e79158e67784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.11.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.11.4-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.0)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
            "Collecting scikit-learn<1.6.0,>=1.1 (from scanpy)\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.15.3)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info2 (from scanpy)\n",
            "  Downloading session_info2-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.13.2)\n",
            "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.7)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.1->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->scanpy) (1.17.0)\n",
            "Downloading scanpy-1.11.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.11.4-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading session_info2-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading array_api_compat-1.12.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: session-info2, legacy-api-wrap, array-api-compat, scikit-learn, anndata, scanpy\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed anndata-0.11.4 array-api-compat-1.12.0 legacy-api-wrap-1.4.1 scanpy-1.11.1 scikit-learn-1.5.2 session-info2-0.1.2\n",
            "Collecting omnipath\n",
            "  Downloading omnipath-1.0.9-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: attrs>=20.2.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (25.3.0)\n",
            "Collecting docrep>=0.3.1 (from omnipath)\n",
            "  Downloading docrep-0.3.2.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: inflect>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (7.5.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from omnipath) (24.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.51.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from omnipath) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (1.17.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from docrep>=0.3.1->omnipath) (1.17.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect>=4.1.0->omnipath) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect>=4.1.0->omnipath) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->omnipath) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->omnipath) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->omnipath) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->omnipath) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->omnipath) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->omnipath) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->omnipath) (2025.4.26)\n",
            "Downloading omnipath-1.0.9-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docrep\n",
            "  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docrep: filename=docrep-0.3.2-py3-none-any.whl size=19876 sha256=65a0be306c3ae032f360a62631d25f71c84001bdd494cb667dbcb697dafd6f9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/76/8f/0ecb7d357c0bff71a2bd1940671be2d07a200752da9189bb55\n",
            "Successfully built docrep\n",
            "Installing collected packages: docrep, omnipath\n",
            "Successfully installed docrep-0.3.2 omnipath-1.0.9\n",
            "Collecting decoupler\n",
            "  Downloading decoupler-1.9.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting numba<0.62.0,>=0.61.0 (from decoupler)\n",
            "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy<3,>=2 in /usr/local/lib/python3.11/dist-packages (from decoupler) (2.0.2)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from decoupler) (2.2.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.11/dist-packages (from decoupler) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from decoupler) (4.13.2)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba<0.62.0,>=0.61.0->decoupler)\n",
            "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.2->decoupler) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.2->decoupler) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.2->decoupler) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.2->decoupler) (1.17.0)\n",
            "Downloading decoupler-1.9.2-py3-none-any.whl (122 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.6/122.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llvmlite, numba, decoupler\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed decoupler-1.9.2 llvmlite-0.44.0 numba-0.61.2\n",
            "Collecting torchbnn\n",
            "  Downloading torchbnn-1.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading torchbnn-1.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: torchbnn\n",
            "Successfully installed torchbnn-1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scanpy\n",
        "!pip install omnipath\n",
        "!pip install decoupler\n",
        "!pip install torchbnn\n",
        "\n",
        "## NEW PACKAGES\n",
        "import os\n",
        "import polars as pl   # install polars!!\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "import torch.distributions\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scanpy as sc\n",
        "import pandas as pd\n",
        "from collections import OrderedDict, defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "import decoupler as dc\n",
        "import omnipath as op\n",
        "\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "V8MUlxTCOHRJ",
        "outputId": "032cb4e0-c081-4027-971c-58de023d1082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-20 22:41:59--  https://docs.google.com/uc?export=download&id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG\n",
            "Resolving docs.google.com (docs.google.com)... 192.178.163.102, 192.178.163.113, 192.178.163.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|192.178.163.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG&export=download [following]\n",
            "--2025-05-20 22:41:59--  https://drive.usercontent.google.com/download?id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.199.132, 2607:f8b0:400e:c05::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.199.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45277554 (43M) [application/octet-stream]\n",
            "Saving to: ‘PBMC_train.h5ad’\n",
            "\n",
            "PBMC_train.h5ad     100%[===================>]  43.18M   183MB/s    in 0.2s    \n",
            "\n",
            "2025-05-20 22:42:03 (183 MB/s) - ‘PBMC_train.h5ad’ saved [45277554/45277554]\n",
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9a32ca738bb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG' -O PBMC_train.h5ad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load data as anndata object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mPBMC_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_h5ad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PBMC_train.h5ad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ],
      "source": [
        "# download PBMC dataset for training\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG' -O PBMC_train.h5ad\n",
        "# load data as anndata object\n",
        "PBMC_train = sc.read_h5ad(\"PBMC_train.h5ad\")\n",
        "\n",
        "# for validation\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1rJKZYIG7rv7BQbDD9RElYOgHcxxzjcYj' -O PBMC_valid.h5ad\n",
        "# load data as anndata object\n",
        "PBMC_valid = sc.read_h5ad( \"PBMC_valid.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wufryazhXHmJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dc.op does not work for me\n",
        "regulons = dc.get_collectri(organism='human', split_complexes=False)\n",
        "\n",
        "filtered_regulons = regulons[regulons['source'].isin(\n",
        "    [\n",
        "      \"GTF2I\",\n",
        "      \"GTF3A\",\n",
        "      \"NRF1\",\n",
        "      \"ELF1\",\n",
        "      \"STAT1\",\n",
        "      \"STAT2\",\n",
        "      \"IRF9\",\n",
        "      \"STAT3\",\n",
        "      \"STAT4\",\n",
        "      \"STAT5A\",\n",
        "      \"STAT5B\",\n",
        "      \"IRF3\",\n",
        "      \"IRF7\",\n",
        "      \"IRF1\",\n",
        "      \"IRF5\",\n",
        "     \"IRF8\",\n",
        "     ])]\n",
        "\n",
        "\"\"\"\n",
        "['IRF1', 'IRF2', 'IRF2BPL', 'IRF3', 'IRF4', 'IRF5', 'IRF6', 'IRF7','IRF8', 'IRF9', 'STAT1', 'STAT2', 'STAT3', 'STAT4', 'STAT5A','STAT5B', 'STAT6','GTF2I','RUNX1', 'MYOD1', 'SOX2','KLF4', 'THAP11','ADNP2', 'AEBP1','AHRR', 'ALX4','APBB1','APEX1', 'ARHGAP35','ARID1A']\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atUwvoywQ0AB"
      },
      "source": [
        "### TODO: NEED TO SET THRESHOLD FOR REGULON SIZE AND FILTER!!!!!!!\n",
        "\n",
        "- median size?\n",
        "- good to restrict the lower bound of regulon size because, essentially we are looking at how much genes are enriched in that TF or pathway. And, if we want a reliable proportion of weights to estimate how much each gene contribute in the pathway, we want enough number of genes regulated by that TF/pathway. So, filter out regulons smaller than 5.\n",
        "- for the upper bound, unless the TF regulates like more than half of the genes in the dataset, it is fine to include them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir5WyFy0YOCl"
      },
      "source": [
        "## Creating gmt files\n",
        "Consider only stimulation or also train it for inhibition as well?\n",
        "But this really depends because the direction of regulation depends on the tissues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJp6LpiyQbuv"
      },
      "source": [
        "# VEGA Architecture\n",
        "\n",
        "**VEGA model**\n",
        "- a variational autoencoder model enhanced by gene annotations\n",
        "- users can provide prior knowledge of gene to the *decoder* e.g. gene regulatory network\n",
        "- allowing the latent variables to be directly interpretable\n",
        "\n",
        "**Encoder**\n",
        "- Two fully connected layer with dropout\n",
        "- Since we are building a variational autoencoder which aims to learn a *probabilistic* latent space, we use a reparameterization trick.\n",
        " - Reparameterization trick is needed because backpropagation is computationally challenging when the latent space is probabilistic\n",
        " - So, we decompose the latent variable into $ z = \\mu+\\epsilon\\cdot\\sigma, \\text{where } \\epsilon \\sim N(0,1)$. Then, gradients can be computed as the random variable $\\epsilon$, independent of learnable parameters of the network ($\\phi$ for encoder, $\\theta$ for decoder).\n",
        "\n",
        "**Decoder**\n",
        "- to achieve biological interpretability, we must provide a gene module that defines how the nodes in decoder layers are connected\n",
        "- this definition is fed to the model by creating a *mask*\n",
        "\n",
        "\n",
        "**Dataset?**\n",
        "- we use CollecTRI dataset to define transcriptional regulatory interactions (which TF targets which genes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks5D7Z4DQyjp"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, dropout, z_dropout): #dropout, z_dropout define the dropout rates of the encoder/latent space\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(input_dims, 800),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(p = dropout),\n",
        "                                     nn.Linear(800, 800),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(p = dropout))  #two layer, fully connected encoder with dropout\n",
        "\n",
        "\n",
        "        self.mu = nn.Sequential(nn.Linear(800, latent_dims),\n",
        "                                nn.Dropout(p = z_dropout))\n",
        "\n",
        "        self.sigma = nn.Sequential(nn.Linear(800, latent_dims),\n",
        "                                   nn.Dropout(p = z_dropout))\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)     # epsilon(error)\n",
        "        self.N.loc = self.N.loc.to(device)\n",
        "        self.N.scale = self.N.scale.to(device)\n",
        "        self.kl = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        mu =  self.mu(x)\n",
        "        sigma = torch.exp(self.sigma(x)) # exp for numeric stability\n",
        "        z = mu + sigma*self.N.sample(mu.shape)  # reparameterization trick\n",
        "        self.kl = (0.5*sigma**2 + 0.5*mu**2 - torch.log(sigma) - 1/2).sum() #calculation of kullback-leibler divergence\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv9NW-tmoWxY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# rows should correspond to genes\n",
        "# cols should correspond to gene sets/TFs\n",
        "def create_mask(pbmc_train, pbmc_val, filtered_regulons, extra_nodes_count=1):\n",
        "    '''\n",
        "    filtered_regulons: collectri regulons filtered to contain only selected TFs\n",
        "    pbmc_train: AnnData object\n",
        "    pbmc_val: AnnData object\n",
        "    extra_nodes_count: number of additional unannotated nodes to add to the mask\n",
        "\n",
        "    Outputs:\n",
        "    mask: torch tensor of shape (n_genes, n_TFs + extra_nodes_count)\n",
        "    mask_df: polars DataFrame with the same shape as mask, containing the TFs as columns and genes as rows\n",
        "    train: AnnData object with the same genes as mask_df\n",
        "    valid: AnnData object with the same genes as mask_df\n",
        "    '''\n",
        "####FOR DTYPE COMPATIBILITY\n",
        "    filtered_regulons = filtered_regulons.astype({\n",
        "        \"source\": str,\n",
        "        \"target\": str,\n",
        "        \"weight\": np.float64\n",
        "    })\n",
        "\n",
        "    tmp = (\n",
        "        pl\n",
        "        .from_pandas(filtered_regulons)\n",
        "        # alice and kerem and yusuf decided to convert all -1 to 1 and remove positive weights restriction\n",
        "        .with_columns(\n",
        "            pl.col(\"weight\").replace(-1, 1)\n",
        "        )\n",
        "        .filter(\n",
        "            pl.col(\"target\").is_in(pbmc_train.var.index.to_numpy())\n",
        "        )  # 901 x 13 -> 332 x 13\n",
        "        .pivot(\n",
        "            on=\"source\",  # new columns\n",
        "            index=\"target\",  # stays in rows\n",
        "            values=\"weight\"\n",
        "        )\n",
        "        .fill_null(0)\n",
        "    )\n",
        "\n",
        "    for i in range(0, extra_nodes_count):\n",
        "        node_index = i + 1\n",
        "        node_name = \"unannotated_\" + str(node_index)\n",
        "        tmp = tmp.with_columns(\n",
        "            pl.lit(1).alias(node_name)\n",
        "        )\n",
        "\n",
        "\n",
        "    train = pbmc_train[:, tmp[\"target\"].to_list()].copy()\n",
        "    valid = pbmc_val[:, tmp[\"target\"].to_list()].copy()\n",
        "\n",
        "    return tmp, train, valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSNBJ0kUQgVg"
      },
      "outputs": [],
      "source": [
        "# define VEGA's decoder\n",
        "\n",
        "class DecoderVEGA(nn.Module):\n",
        "  \"\"\"\n",
        "  Define VEGA's decoder (sparse, one-layer, linear, positive)\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               mask, soft_penalty):\n",
        "        super(DecoderVEGA, self).__init__()\n",
        "\n",
        "        self.sparse_layer = nn.Sequential(SparseLayer(mask, soft_penalty)) # we define the architecture of the decoder below with the class \"SparseLayer\"\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.sparse_layer(x.to(device))\n",
        "    return(z)\n",
        "\n",
        "  def positive_weights(self):\n",
        "      \"\"\"\n",
        "      constrain the decoder to positive weights (set negative weigths to zero)\n",
        "      \"\"\"\n",
        "      w = self.sparse_layer[0].weight\n",
        "      w.data = w.data.clamp(0)\n",
        "      return w\n",
        "\n",
        "\n",
        "# define a class SparseLayer, that specifies the decoder architecture (sparse connections based on the mask)\n",
        "class SparseLayer(nn.Module):\n",
        "  def __init__(self, mask, soft_penalty):\n",
        "        \"\"\"\n",
        "        Extended torch.nn module which mask connection\n",
        "        \"\"\"\n",
        "        super(SparseLayer, self).__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(torch.tensor(mask, dtype=torch.float).t(), requires_grad=False)\n",
        "        self.weight = nn.Parameter(torch.Tensor(mask.shape[1], mask.shape[0]))\n",
        "        self.bias = nn.Parameter(torch.Tensor(mask.shape[1]))\n",
        "        self.soft_penalty = soft_penalty\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "        # mask weight\n",
        "        self.weight.data = self.weight.data * self.mask\n",
        "\n",
        "  def forward(self, input):\n",
        "        # See the autograd section for explanation of what happens here\n",
        "        return SparseLayerFunction.apply(input, self.weight, self.bias, self.mask, self.soft_penalty)\n",
        "\n",
        "\n",
        "######### You don't need to understand this part of the code in detail #########\n",
        "class SparseLayerFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We define our own autograd function which masks it's weights by 'mask'.\n",
        "    For more details, see https://pytorch.org/docs/stable/notes/extending.html\n",
        "    \"\"\"\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias, mask, soft_penalty):\n",
        "        ctx.soft_penalty = soft_penalty\n",
        "        weight = weight * (mask != 0) # change weight to 0 where mask == 0\n",
        "        #calculate the output\n",
        "        output = input.mm(weight.t())\n",
        "        output += bias.unsqueeze(0).expand_as(output) # Add bias to all values in output\n",
        "        ctx.save_for_backward(input, weight, bias, mask)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output): # define the gradient formula\n",
        "        input, weight, bias, mask = ctx.saved_tensors\n",
        "        grad_input = grad_weight = grad_bias = grad_mask = None\n",
        "\n",
        "        # These needs_input_grad checks are optional and only to improve efficiency\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.mm(weight)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().mm(input)\n",
        "            # change grad_weight to 0 where mask == 0\n",
        "            grad_weight = grad_weight * mask\n",
        "        if ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0).squeeze(0)\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias, grad_mask, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVRU0l49Qkvz"
      },
      "outputs": [],
      "source": [
        "class VEGA(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, mask, dropout = 0.3, z_dropout = 0.3):\n",
        "        super(VEGA, self).__init__()\n",
        "        self.encoder = Encoder(latent_dims, input_dims, dropout, z_dropout) # we use the same encoder as before (two-layer, fully connected, non-linear)\n",
        "        self.decoder = DecoderVEGA(mask, soft_penalty=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDRpTVc4J32t"
      },
      "source": [
        "# Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLB1nuEPRewE"
      },
      "outputs": [],
      "source": [
        "### training loop\n",
        "def trainVEGA_with_valid(vae, data, val_data, epochs=100, beta = 0.0001, learning_rate = 0.001):\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr = learning_rate, weight_decay = 5e-4)\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        train_loss_e = 0\n",
        "        valid_loss_e = 0\n",
        "        vae.train() #train mode\n",
        "\n",
        "        for x in data:\n",
        "            x = x.to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            loss = ((x - x_hat)**2).sum() + beta* vae.encoder.kl\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss_e += loss.to('cpu').detach().numpy()\n",
        "            vae.decoder.positive_weights() # we restrict the decoder to positive weights\n",
        "        train_losses.append(train_loss_e/(len(data)*128))\n",
        "\n",
        "        #### Here you should add the validation loop\n",
        "        vae.eval()\n",
        "\n",
        "        for x in val_data:\n",
        "            x = x.to(device)\n",
        "            x_hat = vae(x)\n",
        "            loss = ((x - x_hat)**2).sum() + beta * vae.encoder.kl\n",
        "            valid_loss_e += loss.to('cpu').detach().numpy()\n",
        "        valid_losses.append(valid_loss_e/(len(val_data)*128))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"epoch: \", epoch, \" train_loss: \", train_loss_e/(len(data)*128), \"  valid_loss: \", valid_loss_e/(len(val_data)*128))\n",
        "\n",
        "    return vae, train_losses, valid_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDs7NcJq2PEr"
      },
      "outputs": [],
      "source": [
        "def get_weight(vae):\n",
        "  vae.eval()\n",
        "  return vae.decoder.sparse_layer[0].weight.data.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GATK3dD8TG_d"
      },
      "source": [
        "# TODO: Implement Stochastic Weight Averaging (SWA)\n",
        "\n",
        "* aggregates predictions of multiple models by SWA e.g.\n",
        "  (cond1, cond2, ..) and (ctrl1, ctrl2, ..)\n",
        "* can yield more robust weights\n",
        "* SWA performs an equal average of the weights traversed by SGD with a modified learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vixLP-D64ZKR"
      },
      "outputs": [],
      "source": [
        "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
        "\n",
        "def trainVEGA_with_swa(vae, data, val_data, epochs=100, beta=0.0001,\n",
        "                       learning_rate=0.001, swa_start=75, swa_lr=0.05):\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
        "    swa_model = AveragedModel(vae)           # will hold the running average\n",
        "    swa_scheduler = SWALR(opt, swa_lr=swa_lr)\n",
        "\n",
        "    train_losses, valid_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        train_loss_e = 0.\n",
        "        for x in data:\n",
        "            x = x.to(device)\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            loss = ((x - x_hat)**2).sum() + beta * vae.encoder.kl\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            vae.decoder.positive_weights()\n",
        "            train_loss_e += loss.detach().cpu().item()\n",
        "\n",
        "        # start updating our SWA weights\n",
        "        if epoch >= swa_start:\n",
        "            swa_model.update_parameters(vae)\n",
        "            swa_scheduler.step()\n",
        "\n",
        "        train_losses.append(train_loss_e / (len(data) * data.batch_size))\n",
        "\n",
        "        # validation\n",
        "        vae.eval()\n",
        "        valid_loss_e = 0.\n",
        "        with torch.no_grad():\n",
        "            for x in val_data:\n",
        "                x = x.to(device)\n",
        "                x_hat = vae(x)\n",
        "                loss = ((x - x_hat)**2).sum() + beta * vae.encoder.kl\n",
        "                valid_loss_e += loss.detach().cpu().item()\n",
        "        valid_losses.append(valid_loss_e / (len(val_data) * val_data.batch_size))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: train_loss={train_losses[-1]:.4f}, valid_loss={valid_losses[-1]:.4f}\")\n",
        "\n",
        "    # recompute batch‐norm statistics for the SWA model (if you had any)\n",
        "    update_bn(data, swa_model, device=device)\n",
        "\n",
        "    return swa_model, train_losses, valid_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh-BRDYq4c9p"
      },
      "outputs": [],
      "source": [
        "def get_weight_swa(swa_model):\n",
        "    \"\"\"\n",
        "    Extract the sparse‐decoder weights from an AveragedModel.\n",
        "    \"\"\"\n",
        "    # if it's wrapped in AveragedModel, the averaged params live in swa_model.module\n",
        "    model = swa_model.module if hasattr(swa_model, \"module\") else swa_model\n",
        "    # pull out the sparse‐layer weight matrix\n",
        "    W = model.decoder.sparse_layer[0].weight.data.cpu().numpy()\n",
        "    return W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlM-gGG-TWJ0"
      },
      "source": [
        "# TODO: Apply Bayesian layer\n",
        "\n",
        "* replace nn.Linear in Encoder class with bnn.BayesLinear\n",
        "* use nn.MSELoss() and bnn.BKLLoss() for optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef6acgwDLeKA"
      },
      "outputs": [],
      "source": [
        "def _kl_loss(mu_0, log_sigma_0, mu_1, log_sigma_1) :\n",
        "    \"\"\"\n",
        "    An method for calculating KL divergence between two Normal distribtuion.\n",
        "\n",
        "    Arguments:\n",
        "        mu_0 (Float) : mean of normal distribution.\n",
        "        log_sigma_0 (Float): log(standard deviation of normal distribution).\n",
        "        mu_1 (Float): mean of normal distribution.\n",
        "        log_sigma_1 (Float): log(standard deviation of normal distribution).\n",
        "\n",
        "    \"\"\"\n",
        "    kl = log_sigma_1 - log_sigma_0 + \\\n",
        "    (torch.exp(log_sigma_0)**2 + (mu_0-mu_1)**2)/(2*math.exp(log_sigma_1)**2) - 0.5\n",
        "    return kl.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1yPQTmwS88O"
      },
      "outputs": [],
      "source": [
        "class BayesDecoder(nn.Module):\n",
        "    def __init__(self, mask, soft_penalty):\n",
        "        super(BayesDecoder, self).__init__()\n",
        "        self.sparse_layer = nn.Sequential(BayesianSparseLayer(mask, soft_penalty))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sparse_layer(x.to(device))\n",
        "\n",
        "    def kl_divergence(self):\n",
        "        return self.sparse_layer[0].kl_loss()\n",
        "\n",
        "    def positive_weights(self):\n",
        "      \"\"\"\n",
        "      constrain the decoder to positive weights (set negative weigths to zero)\n",
        "      \"\"\"\n",
        "      w = self.sparse_layer[0].weight_mu\n",
        "      w.data = w.data.clamp(0)\n",
        "      return w\n",
        "\n",
        "\n",
        "class BayesianSparseLayer(nn.Module):\n",
        "    def __init__(self, mask, soft_penalty):\n",
        "        super(BayesianSparseLayer, self).__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(torch.tensor(mask, dtype=torch.float).t(), requires_grad=False)\n",
        "        self.in_features = mask.shape[0]\n",
        "        self.out_features = mask.shape[1]\n",
        "\n",
        "        self.prior_mu = 0\n",
        "        self.prior_sigma = 0.1\n",
        "        self.prior_log_sigma = math.log(0.1)\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(self.out_features, self.in_features))\n",
        "        self.weight_log_sigma = nn.Parameter(torch.Tensor(self.out_features, self.in_features))\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(self.out_features))\n",
        "        self.bias_log_sigma = nn.Parameter(torch.Tensor(self.out_features))\n",
        "\n",
        "        self.soft_penalty = soft_penalty\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Initialization method of Adv-BNN\n",
        "        stdv = 1. / math.sqrt(self.weight_mu.size(1))\n",
        "        self.weight_mu.data.uniform_(-stdv, stdv)\n",
        "        self.weight_log_sigma.data.fill_(self.prior_log_sigma)\n",
        "        self.bias_mu.data.uniform_(-stdv, stdv)\n",
        "        self.bias_log_sigma.data.fill_(self.prior_log_sigma)\n",
        "        # mask weight\n",
        "        self.weight_mu.data = self.weight_mu.data * self.mask\n",
        "        self.weight_log_sigma.data = self.weight_log_sigma * self.mask\n",
        "\n",
        "    def forward(self, input):\n",
        "        weight = self.weight_mu + torch.exp(self.weight_log_sigma) * torch.randn_like(self.weight_log_sigma)\n",
        "        bias = self.bias_mu + torch.exp(self.bias_log_sigma) * torch.randn_like(self.bias_log_sigma)\n",
        "        return SparseLayerFunction.apply(input, weight, bias, self.mask, self.soft_penalty)\n",
        "\n",
        "    def kl_loss(self):\n",
        "        return _kl_loss(self.weight_mu, self.weight_log_sigma,\n",
        "                        self.prior_mu, self.prior_log_sigma)\n",
        "\n",
        "class BayesVEGA(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, mask, dropout = 0.3, z_dropout = 0.3, soft_penalty = 0.1):\n",
        "        super(BayesVEGA, self).__init__()\n",
        "        self.encoder = Encoder(latent_dims, input_dims, dropout, z_dropout) # we use the same encoder as before (two-layer, fully connected, non-linear)\n",
        "        self.decoder = BayesDecoder(mask, soft_penalty)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT0yEo0mLi6r"
      },
      "outputs": [],
      "source": [
        "def get_weight_bayes(model):\n",
        "    # pull out the sparse‐layer weight matrix\n",
        "    W = model.decoder.sparse_layer[0].weight_mu.data.cpu().numpy()\n",
        "    return W\n",
        "\n",
        "def get_weight_uncertainties_bayes(model):\n",
        "    # pull out the sparse‐layer weight matrix\n",
        "    W = model.decoder.sparse_layer[0].weight_log_sigma.data.cpu().numpy()\n",
        "    return W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxiggu7VLmAx"
      },
      "outputs": [],
      "source": [
        "### training loop\n",
        "def trainVEGA_with_bayes(vae, data, val_data, epochs=100, beta_en = 0.0001, beta_de = 0.0001, learning_rate = 0.001):\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr = learning_rate, weight_decay = 5e-4)\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        train_loss_e = 0\n",
        "        valid_loss_e = 0\n",
        "        vae.train() #train mode\n",
        "\n",
        "        for x in data:\n",
        "            x = x.to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            kl_encoder = vae.encoder.kl\n",
        "            kl_decoder = vae.decoder.kl_divergence()\n",
        "\n",
        "            loss = ((x - x_hat)**2).sum() + beta_en* kl_encoder + beta_de*kl_decoder\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss_e += loss.to('cpu').detach().numpy()\n",
        "            vae.decoder.positive_weights() # we restrict the decoder to positive weights\n",
        "        train_losses.append(train_loss_e/(len(data)*128))\n",
        "\n",
        "        #### Here you should add the validation loop\n",
        "        vae.eval()\n",
        "\n",
        "        for x in val_data:\n",
        "            x = x.to(device)\n",
        "            x_hat = vae(x)\n",
        "            kl_encoder = vae.encoder.kl\n",
        "            kl_decoder = vae.decoder.kl_divergence()\n",
        "            loss = ((x - x_hat)**2).sum() + beta_en* kl_encoder + beta_de*kl_decoder\n",
        "            valid_loss_e += loss.to('cpu').detach().numpy()\n",
        "        valid_losses.append(valid_loss_e/(len(val_data)*128))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"epoch: \", epoch, \" train_loss: \", train_loss_e/(len(data)*128), \"  valid_loss: \", valid_loss_e/(len(val_data)*128))\n",
        "\n",
        "    return vae, train_losses, valid_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snkem2xNR-Xo"
      },
      "source": [
        "# Split train/valid dataset\n",
        "We need to train `PBMC_train_ctrl` and `PBMC_train_stim` separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x545IMNEQuKg",
        "outputId": "d56fda79-bb63-4b07-9f9e-0cd0f772d152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PBMC_train_stim:  7109\n",
            "PBMC_train_ctrl:  6406\n",
            "PBMC_valid_stim:  1777\n",
            "PBMC_valid_ctrl:  1601\n"
          ]
        }
      ],
      "source": [
        "# Split data set into control and stimulated\n",
        "PBMC_train_stim = PBMC_train[PBMC_train.obs[\"condition\"] == \"stimulated\"]\n",
        "print(\"PBMC_train_stim: \", len(PBMC_train_stim))\n",
        "PBMC_train_ctrl = PBMC_train[PBMC_train.obs[\"condition\"] == \"control\"]\n",
        "print(\"PBMC_train_ctrl: \", len(PBMC_train_ctrl))\n",
        "PBMC_valid_stim = PBMC_valid[PBMC_valid.obs[\"condition\"] == \"stimulated\"]\n",
        "print(\"PBMC_valid_stim: \", len(PBMC_valid_stim))\n",
        "PBMC_valid_ctrl = PBMC_valid[PBMC_valid.obs[\"condition\"] == \"control\"]\n",
        "print(\"PBMC_valid_ctrl: \",len(PBMC_valid_ctrl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J32_efKZTEM5"
      },
      "source": [
        "# TODO: Train basic model (with activator or inhibitor gmt) on vega, swa, bayes\n",
        "Train each model (cond/ctrl) multiple times (and then compute metrics such as mean and standard deviation for the different weights). Are there genes whose annotation to a given TF seems to be \"less certain\" than for others?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzlDn7gESFux"
      },
      "outputs": [],
      "source": [
        "def save_losses(train_losses, valid_losses, path, condition):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    with open(os.path.join(path, f\"{condition}, losses.csv\"), \"w\") as f:\n",
        "        f.write(\"epoch,train_loss,valid_loss\\n\")\n",
        "        for epoch, (train_loss, valid_loss) in enumerate(zip(train_losses, valid_losses)):\n",
        "            f.write(f\"{epoch},{train_loss},{valid_loss}\\n\")\n",
        "\n",
        "def run_vega_model(model_type, train_data, valid_data, path_to_save = 'models/vega', cond = 'all', cell_type = 'all', N=10, epochs = 60): # I need this encoder argument\n",
        "    \"\"\"\n",
        "    cond is \"stimulated\" or \"control\" or 'all' (I need 'all' to train the encoder before frezeing it)\n",
        "    model type is 'bayes', 'vega' or 'swa'\n",
        "    \"\"\"\n",
        "\n",
        "    mask_df, train_data, valid_data = create_mask(train_data, valid_data, filtered_regulons)\n",
        "\n",
        "    # convert mask polars to DF\n",
        "    numeric_columns = [\n",
        "    name for name, dtype in mask_df.schema.items()\n",
        "    if dtype in (pl.Float32, pl.Float64, pl.Int32, pl.Int64)\n",
        "    ]\n",
        "    mask = mask_df.select(numeric_columns).to_numpy()\n",
        "    mask = torch.from_numpy(mask).float()\n",
        "\n",
        "    # create mask\n",
        "    if not cond == 'all':\n",
        "        train_data = train_data[train_data.obs[\"condition\"] == cond]\n",
        "        valid_data = valid_data[valid_data.obs[\"condition\"] == cond]\n",
        "\n",
        "    if not cell_type == 'all':\n",
        "        train_data = train_data[train_data.obs[\"cell_type\"] == cell_type]\n",
        "        valid_data = valid_data[valid_data.obs[\"cell_type\"] == cell_type]\n",
        "\n",
        "    # set up dataloader\n",
        "    trainX = torch.utils.data.DataLoader(train_data.X.toarray(), batch_size=128, shuffle=True)\n",
        "    validX = torch.utils.data.DataLoader(valid_data.X.toarray(), batch_size=128, shuffle=True)\n",
        "\n",
        "    all_weights = []\n",
        "\n",
        "    match model_type:\n",
        "        case \"vega\":\n",
        "            for _ in range(N):\n",
        "                os.makedirs(path_to_save, exist_ok=True)\n",
        "                vega = VEGA(latent_dims= mask.shape[1], input_dims = mask.shape[0], mask = mask.T, dropout = 0.3, z_dropout = 0.3).to(device)    #   REPLACE THIS WITH YOUR VEGA MODEL\n",
        "                vega, train_losses, valid_losses = trainVEGA_with_valid(vega, trainX, validX, epochs=epochs, beta=0.0001)\n",
        "                weight = get_weight(vega)\n",
        "                all_weights.append(weight)\n",
        "                save_losses(train_losses, valid_losses, path_to_save, cond)  # there is no need to plot it, let's save a loss function to plot it later\n",
        "\n",
        "        case \"bayes\":\n",
        "            weight_uncertainties = []\n",
        "            for _ in range(N):\n",
        "                os.makedirs(path_to_save, exist_ok=True)\n",
        "                vega = BayesVEGA(latent_dims= mask.shape[1], input_dims = mask.shape[0], mask = mask.T, dropout = 0.3,  z_dropout = 0.3).to(device)          #   REPLACE THIS WITH YOUR VEGA MODEL\n",
        "                vega, train_losses, valid_losses = trainVEGA_with_bayes(vega, trainX, validX, epochs = epochs, beta_en = 0.0001, beta_de=0.0001)\n",
        "                weight = get_weight_bayes(vega)\n",
        "                uncertainty = get_weight_uncertainties_bayes(vega)\n",
        "                uncertainty = np.exp(uncertainty)\n",
        "                all_weights.append(weight)\n",
        "                weight_uncertainties.append(uncertainty)\n",
        "                save_losses(train_losses, valid_losses, path_to_save, cond)\n",
        "\n",
        "            weight_uncertainties = np.stack(weight_uncertainties)\n",
        "            mean_weight_uncertainties = weight_uncertainties.mean(axis=0)\n",
        "            mean_weight_uncertainties = pd.DataFrame(mean_weight_uncertainties, index = train_data.var.index.tolist(), columns=mask_df.columns[1:])\n",
        "            mean_weight_uncertainties.to_csv(os.path.join(path_to_save, f\"{cond}_{model_type}_{cell_type}_weight_uncertainty_mean.csv\"), index=True)\n",
        "\n",
        "        case \"swa\":\n",
        "            for _ in range(N):\n",
        "                os.makedirs(path_to_save, exist_ok=True)\n",
        "                vega = VEGA(latent_dims=mask.shape[1],\n",
        "                        input_dims=mask.shape[0],\n",
        "                        mask=mask.T,\n",
        "                        dropout=0.3,\n",
        "                        z_dropout=0.3).to(device)\n",
        "\n",
        "                swa_model, train_losses, valid_losses = trainVEGA_with_swa(\n",
        "                    vega, trainX, validX,\n",
        "                    epochs=100, beta=0.0001,\n",
        "                    learning_rate=0.0005,\n",
        "                    swa_start=80,\n",
        "                    swa_lr=0.0001\n",
        "                )\n",
        "\n",
        "                W_swa = get_weight_swa(swa_model)\n",
        "                all_weights.append(W_swa)\n",
        "                save_losses(train_losses, valid_losses, path_to_save, cond)\n",
        "\n",
        "\n",
        "    stacked = np.stack(all_weights)\n",
        "    mean_weight = stacked.mean(axis=0)\n",
        "    std_weight = stacked.std(axis=0)\n",
        "\n",
        "    mean_weight = pd.DataFrame(mean_weight, index = train_data.var.index.tolist(), columns=mask_df.columns[1:])\n",
        "    std_weight = pd.DataFrame(std_weight, index = train_data.var.index.tolist(), columns=mask_df.columns[1:])\n",
        "\n",
        "\n",
        "    mean_weight.to_csv(os.path.join(path_to_save, f\"{cond}_{model_type}_{cell_type}_weight_mean.csv\"), index=True)\n",
        "    std_weight.to_csv(os.path.join(path_to_save, f\"{cond}_{model_type}_{cell_type}_weight_std.csv\"), index=True)\n",
        "\n",
        "    return vega, mean_weight, std_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7eS64vSXevR"
      },
      "outputs": [],
      "source": [
        "vega, mean_weight, std_weight = run_vega_model(\"vega\", PBMC_train_stim, PBMC_valid_stim, path_to_save = 'models/vega', cond = 'stimulated', cell_type = 'all', N=1, epochs = 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZsdYD_oUqD7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_tf_rewiring(path, tf_names, title,\n",
        "    n_cols=3, figsize=(15, 12)\n",
        "):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for filename in files:\n",
        "            print(f\"Processing file: {filename}\")\n",
        "            if 'bayes' in filename:\n",
        "                if filename.endswith('uncertainty_mean.csv'):\n",
        "                    if 'control' in filename:\n",
        "                        file_path = os.path.join(root, filename)\n",
        "                        sigma_weights_ctrl = pd.read_csv(file_path, index_col=0)\n",
        "                    elif 'stimulated' in filename:\n",
        "                        file_path = os.path.join(root, filename)\n",
        "                        sigma_weights_stim = pd.read_csv(file_path, index_col=0)\n",
        "            else:\n",
        "                if filename.endswith('std.csv'):\n",
        "                    if 'control' in filename:\n",
        "                        file_path = os.path.join(root, filename)\n",
        "                        sigma_weights_ctrl = pd.read_csv(file_path, index_col=0)\n",
        "                    elif 'stimulated' in filename:\n",
        "                        file_path = os.path.join(root, filename)\n",
        "                        sigma_weights_stim = pd.read_csv(file_path, index_col=0)\n",
        "\n",
        "            if filename.endswith('weight_mean.csv'):\n",
        "                if 'control' in filename:\n",
        "                    file_path = os.path.join(root, filename)\n",
        "                    mean_weights_ctrl = pd.read_csv(file_path, index_col=0)\n",
        "                elif 'stimulated' in filename:\n",
        "                    file_path = os.path.join(root, filename)\n",
        "                    mean_weights_stim = pd.read_csv(file_path, index_col=0)\n",
        "\n",
        "    n_plots = len(tf_names)\n",
        "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axs = plt.subplots(n_rows, n_cols, figsize=figsize, squeeze=False)\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    for idx, tf_name in enumerate(tf_names):\n",
        "        row, col = divmod(idx, n_cols)\n",
        "        ax = axs[row][col]\n",
        "\n",
        "        # Filter TF target genes\n",
        "        stim = mean_weights_stim[tf_name].rename(\"stim\")\n",
        "        ctrl = mean_weights_ctrl[tf_name].rename(\"ctrl\")\n",
        "\n",
        "        std_stim = sigma_weights_stim[tf_name].rename(\"std_stim\")\n",
        "        std_ctrl = sigma_weights_ctrl[tf_name].rename(\"std_ctrl\")\n",
        "\n",
        "        merged = pd.concat([ctrl, stim, std_ctrl, std_stim], axis=1).dropna()\n",
        "        merged[\"uncertainty\"] = merged[[\"std_ctrl\",\"std_stim\"]].mean(axis=1)\n",
        "\n",
        "        ax.errorbar(\n",
        "            merged[\"ctrl\"], merged[\"stim\"],\n",
        "            xerr=merged[\"std_ctrl\"], yerr=merged[\"std_stim\"],\n",
        "            fmt='.',                 # circle markers\n",
        "            ecolor='black',           # color of error bars\n",
        "            elinewidth=0.5,          # thin error bars\n",
        "            markersize=5,\n",
        "            markeredgecolor='blue',\n",
        "            markeredgewidth=0.5\n",
        "        )\n",
        "\n",
        "        ax.plot([merged[\"ctrl\"].min(), merged[\"ctrl\"].max()],\n",
        "        [merged[\"ctrl\"].min(), merged[\"ctrl\"].max()],\n",
        "        color='grey', linestyle='--')\n",
        "        ax.set_xlabel(\"Control mean weight\")\n",
        "        ax.set_ylabel(\"Stimulated mean weight\")\n",
        "        ax.set_title(f\"{tf_name} weight: control vs. stimulated\")\n",
        "\n",
        "        merged[\"dist\"] = np.abs(merged[\"stim\"] - merged[\"ctrl\"]) / np.sqrt(2)\n",
        "        top = merged.nlargest(20, \"dist\")\n",
        "        for gene, row in top.iterrows():\n",
        "            ax.annotate(gene, (row[\"ctrl\"], row[\"stim\"]), fontsize=8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_tf_rewiring(\n",
        "    path=\"./model/bayes/\",\n",
        "    tf_names=[\"STAT1\", \"STAT2\"],\n",
        "    title=\"TF Rewiring: Control vs Stimulated\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4oya_M17UEw"
      },
      "source": [
        "# Train model with merged mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cam9BRvK7afq"
      },
      "outputs": [],
      "source": [
        "def create_merged_mask(adata, activator_gmt, inhibitor_gmt, add_nodes:int=1, sep='\\t'):\n",
        "\n",
        "    activator_dict = OrderedDict()\n",
        "    if activator_gmt is not None:\n",
        "        with open(activator_gmt) as f:\n",
        "            for line in f.readlines():\n",
        "                line = line.strip()\n",
        "                spli = line.split(sep)\n",
        "                activator_dict[spli[0]] = spli[2:]\n",
        "\n",
        "    inhibitor_dict = OrderedDict()\n",
        "    if inhibitor_gmt is not None:\n",
        "        with open(inhibitor_gmt) as f:\n",
        "            for line in f.readlines():\n",
        "                line = line.strip()\n",
        "                spli = line.split(sep)\n",
        "                inhibitor_dict[spli[0]] = spli[2:]\n",
        "\n",
        "    feature_list = adata.var.index.tolist()\n",
        "\n",
        "    unique_keys = set(activator_dict.keys()) | set(inhibitor_dict.keys())\n",
        "\n",
        "    # Create mask\n",
        "    mask = np.zeros((len(feature_list), len(unique_keys)))\n",
        "    for j, k in enumerate(activator_dict.keys()):\n",
        "        for i in range(mask.shape[0]):\n",
        "            if feature_list[i] in activator_dict[k]:      # match genes\n",
        "                mask[i,j] = 1.      # 1 means target is stimulated by TF, 0 means target is not affected by TF\n",
        "\n",
        "    for j, k in enumerate(inhibitor_dict.keys()):\n",
        "        for i in range(mask.shape[0]):\n",
        "            if feature_list[i] in inhibitor_dict[k]:      # match genes\n",
        "                mask[i,j] = -1.\n",
        "\n",
        "     # Add unannotated nodes\n",
        "    vec = np.ones((mask.shape[0], add_nodes))\n",
        "    mask = np.hstack((mask, vec))\n",
        "\n",
        "    adata = adata.copy()\n",
        "    adata.uns['_vega'] = dict() #create attribute \"_vega\" to store the mask and pathway information\n",
        "    adata.uns['_vega']['mask'] = mask\n",
        "    adata.uns['_vega']['gmv_names'] = list(unique_keys) + ['UNANNOTATED_'+str(k) for k in range(add_nodes)]\n",
        "\n",
        "    return adata, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itg29AcgL5_W"
      },
      "source": [
        "### 1. Basic model + merged mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld8BaKVc7fcg"
      },
      "outputs": [],
      "source": [
        "class MergedDecoderVEGA(nn.Module):\n",
        "    \"\"\"\n",
        "    Define VEGA's decoder (sparse, one-layer, linear, positive)\n",
        "    \"\"\"\n",
        "    def __init__(self, mask, soft_penalty):\n",
        "        super(MergedDecoderVEGA, self).__init__()\n",
        "\n",
        "        self.sparse_layer = nn.Sequential(MergedSparseLayer(mask, soft_penalty)) # we define the architecture of the decoder below with the class \"SparseLayer\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.sparse_layer(x.to(device))\n",
        "        return(z)\n",
        "\n",
        "# define a class SparseLayer, that specifies the decoder architecture (sparse connections based on the mask)\n",
        "class MergedSparseLayer(SparseLayer):\n",
        "    def __init__(self, mask, soft_penalty):\n",
        "        \"\"\"\n",
        "        Extended torch.nn module which mask connection\n",
        "        \"\"\"\n",
        "        super().__init__(mask, soft_penalty)\n",
        "\n",
        "        self.activator_mask = (self.mask == 1).float()\n",
        "        self.inhibitor_mask = (self.mask == -1).float()\n",
        "        self.zero_mask = (self.mask == 0).float()\n",
        "        self.soft_penalty = self.soft_penalty\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def constrain_weights(self):\n",
        "        with torch.no_grad():\n",
        "            # Clamp activator weights to be >= 0\n",
        "            device = self.weight.device\n",
        "\n",
        "            a_mask = self.activator_mask.to(device)\n",
        "            i_mask = self.inhibitor_mask.to(device)\n",
        "            z_mask = self.zero_mask.to(device)\n",
        "\n",
        "            self.weight.data = self.weight.data * (1 - a_mask) + \\\n",
        "                               self.weight.data.clamp(min=0) * a_mask\n",
        "\n",
        "            # Clamp inhibitor weights to be <= 0\n",
        "            self.weight.data = self.weight.data * (1 - i_mask) + \\\n",
        "                               self.weight.data.clamp(max=0) * i_mask\n",
        "\n",
        "            # Set zero-regulation weights to exactly zero\n",
        "            self.weight.data = self.weight.data * (1 - z_mask)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # See the autograd section for explanation of what happens here\n",
        "        return MergedSparseLayerFunction.apply(input, self.weight, self.bias, self.mask, self.soft_penalty)\n",
        "\n",
        "\n",
        "######### You don't need to understand this part of the code in detail #########\n",
        "class MergedSparseLayerFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We define our own autograd function which masks it's weights by 'mask'.\n",
        "    For more details, see https://pytorch.org/docs/stable/notes/extending.html\n",
        "    \"\"\"\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias, mask, soft_penalty=0.1):\n",
        "        ctx.soft_penalty = soft_penalty\n",
        "        return SparseLayerFunction.forward(ctx, input, weight, bias, mask, soft_penalty)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output): # define the gradient formula\n",
        "        input, weight, bias, mask = ctx.saved_tensors\n",
        "        soft_penalty = ctx.soft_penalty\n",
        "        grad_input = grad_weight = grad_bias = grad_mask = None\n",
        "\n",
        "        # These needs_input_grad checks are optional and only to improve efficiency\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.mm(weight * (mask != 0))\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            raw_grad_weight = grad_output.t().mm(input)\n",
        "\n",
        "            grad_weight = torch.zeros_like(raw_grad_weight)\n",
        "\n",
        "            # Activators (mask = 1): allow positive weights, penalize negatives\n",
        "            activator_mask = (mask == 1)\n",
        "            grad_weight += activator_mask * raw_grad_weight\n",
        "            grad_weight -= activator_mask * (weight < 0).float() * weight.abs() * soft_penalty  # soft penalty\n",
        "\n",
        "            # Inhibitors (mask = -1): allow negative weights, penalize positives\n",
        "            inhibitor_mask = (mask == -1)\n",
        "            grad_weight += inhibitor_mask * raw_grad_weight\n",
        "            grad_weight += inhibitor_mask * (weight > 0).float() * weight.abs() * soft_penalty  # soft penalty\n",
        "\n",
        "            # Zeros: do not update\n",
        "            grad_weight *= (mask != 0).float()\n",
        "\n",
        "        if ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0).squeeze(0)\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCTfomG87hjC"
      },
      "outputs": [],
      "source": [
        "class MergedVEGA(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, mask, dropout = 0.3, z_dropout = 0.3, soft_penalty = 0.1):\n",
        "        super(MergedVEGA, self).__init__()\n",
        "        self.encoder = Encoder(latent_dims, input_dims, dropout, z_dropout) # we use the same encoder as before (two-layer, fully connected, non-linear)\n",
        "        self.decoder = MergedDecoderVEGA(mask, soft_penalty)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "944tmlCfMJmG"
      },
      "outputs": [],
      "source": [
        "### training loop\n",
        "def trainMergedVEGA_with_valid(vae, data, val_data, epochs=100, beta = 0.0001, learning_rate = 0.001):\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr = learning_rate, weight_decay = 5e-4)\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        train_loss_e = 0\n",
        "        valid_loss_e = 0\n",
        "        vae.train() #train mode\n",
        "\n",
        "        for x in data:\n",
        "            x = x.to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            loss = ((x - x_hat)**2).sum() + beta* vae.encoder.kl\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss_e += loss.to('cpu').detach().numpy()\n",
        "            vae.decoder.sparse_layer[0].constrain_weights() # we restrict the decoder to positive weights\n",
        "        train_losses.append(train_loss_e/(len(data)*128))\n",
        "\n",
        "        #### Here you should add the validation loop\n",
        "        vae.eval()\n",
        "\n",
        "        for x in val_data:\n",
        "            x = x.to(device)\n",
        "            x_hat = vae(x)\n",
        "            loss = ((x - x_hat)**2).sum() + beta * vae.encoder.kl\n",
        "            valid_loss_e += loss.to('cpu').detach().numpy()\n",
        "        valid_losses.append(valid_loss_e/(len(val_data)*128))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"epoch: \", epoch, \" train_loss: \", train_loss_e/(len(data)*128), \"  valid_loss: \", valid_loss_e/(len(val_data)*128))\n",
        "\n",
        "    return vae, train_losses, valid_losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ6fI9hF7pei"
      },
      "outputs": [],
      "source": [
        "merged_vega_stim, merged_mean_weight_stim, merged_std_weight_stim = run_merged_vega_model(PBMC_train_stim, PBMC_valid_stim, \"stimulated\", \"activator.gmt\", \"inhibitor.gmt\", 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGqv11wF7wXx"
      },
      "outputs": [],
      "source": [
        "merged_vega_ctrl, merged_mean_weight_ctrl, merged_std_weight_ctrl = run_merged_vega_model(PBMC_train_ctrl, PBMC_valid_ctrl, \"control\", \"activator.gmt\", \"inhibitor.gmt\", 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atUNXshAMNZl"
      },
      "source": [
        "### 2. SWA model + merged mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm_YD6VeMRBm"
      },
      "outputs": [],
      "source": [
        "def trainMergedVEGA_with_swa(vae, data, val_data, epochs=100, beta=0.0001,\n",
        "                       learning_rate=0.001, swa_start=75, swa_lr=0.05):\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
        "    swa_model = AveragedModel(vae)           # will hold the running average\n",
        "    swa_scheduler = SWALR(opt, swa_lr=swa_lr)\n",
        "\n",
        "    train_losses, valid_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        train_loss_e = 0.\n",
        "        for x in data:\n",
        "            x = x.to(device)\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            loss = ((x - x_hat)**2).sum() + beta * vae.encoder.kl\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            vae.decoder.sparse_layer[0].constrain_weights()\n",
        "            train_loss_e += loss.detach().cpu().item()\n",
        "\n",
        "        # start updating our SWA weights\n",
        "        if epoch >= swa_start:\n",
        "            swa_model.update_parameters(vae)\n",
        "            swa_scheduler.step()\n",
        "\n",
        "        train_losses.append(train_loss_e / (len(data) * data.batch_size))\n",
        "\n",
        "        # validation\n",
        "        vae.eval()\n",
        "        valid_loss_e = 0.\n",
        "        with torch.no_grad():\n",
        "            for x in val_data:\n",
        "                x = x.to(device)\n",
        "                x_hat = vae(x)\n",
        "                loss = ((x - x_hat)**2).sum() + beta * vae.encoder.kl\n",
        "                valid_loss_e += loss.detach().cpu().item()\n",
        "        valid_losses.append(valid_loss_e / (len(val_data) * val_data.batch_size))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: train_loss={train_losses[-1]:.4f}, valid_loss={valid_losses[-1]:.4f}\")\n",
        "\n",
        "    # recompute batch‐norm statistics for the SWA model (if you had any)\n",
        "    update_bn(data, swa_model, device=device)\n",
        "\n",
        "    return swa_model, train_losses, valid_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivw5GcEgMS1c"
      },
      "source": [
        "### 3. Bayes model + merged mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeRSx2NaMXBr"
      },
      "outputs": [],
      "source": [
        "class MergedBayesDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Define VEGA's decoder (sparse, one-layer, linear, positive)\n",
        "    \"\"\"\n",
        "    def __init__(self, mask, soft_penalty):\n",
        "        super(MergedBayesDecoder, self).__init__()\n",
        "\n",
        "        self.sparse_layer = nn.Sequential(MergedBayesianSparseLayer(mask, soft_penalty)) # we define the architecture of the decoder below with the class \"SparseLayer\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.sparse_layer(x.to(device))\n",
        "        return(z)\n",
        "\n",
        "    def kl_divergence(self):\n",
        "        return self.sparse_layer[0].kl_loss()\n",
        "\n",
        "# define a class SparseLayer, that specifies the decoder architecture (sparse connections based on the mask)\n",
        "class MergedBayesianSparseLayer(BayesianSparseLayer):\n",
        "    def __init__(self, mask, soft_penalty):\n",
        "        \"\"\"\n",
        "        Extended torch.nn module which mask connection\n",
        "        \"\"\"\n",
        "        super(MergedBayesianSparseLayer, self).__init__(mask, soft_penalty)\n",
        "\n",
        "        self.activator_mask = (self.mask == 1).float()\n",
        "        self.inhibitor_mask = (self.mask == -1).float()\n",
        "        self.zero_mask = (self.mask == 0).float()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Initialization method of Adv-BNN\n",
        "        stdv = 1. / math.sqrt(self.weight_mu.size(1))\n",
        "        self.weight_mu.data.uniform_(-stdv, stdv)\n",
        "        self.weight_log_sigma.data.fill_(self.prior_log_sigma)\n",
        "        self.bias_mu.data.uniform_(-stdv, stdv)\n",
        "        self.bias_log_sigma.data.fill_(self.prior_log_sigma)\n",
        "\n",
        "    def constrain_weights(self):\n",
        "        with torch.no_grad():\n",
        "            # Clamp activator weights to be >= 0\n",
        "            device = self.weight_mu.device\n",
        "\n",
        "            a_mask = self.activator_mask.to(device)\n",
        "            i_mask = self.inhibitor_mask.to(device)\n",
        "            z_mask = self.zero_mask.to(device)\n",
        "\n",
        "            self.weight_mu.data = self.weight_mu.data * (1 - a_mask) + \\\n",
        "                               self.weight_mu.data.clamp(min=0) * a_mask\n",
        "\n",
        "            # Clamp inhibitor weights to be <= 0\n",
        "            self.weight_mu.data = self.weight_mu.data * (1 - i_mask) + \\\n",
        "                               self.weight_mu.data.clamp(max=0) * i_mask\n",
        "\n",
        "            # Set zero-regulation weights to exactly zero\n",
        "            self.weight_mu.data = self.weight_mu.data * (1 - z_mask)\n",
        "\n",
        "\n",
        "class MergedBayesVEGA(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, mask, dropout = 0.3, z_dropout = 0.3, soft_penalty = 0.0001):\n",
        "        super(MergedBayesVEGA, self).__init__()\n",
        "        self.encoder = Encoder(latent_dims, input_dims, dropout, z_dropout) # we use the same encoder as before (two-layer, fully connected, non-linear)\n",
        "        self.decoder = MergedBayesDecoder(mask, soft_penalty)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx6E_ImXMZ4x"
      },
      "outputs": [],
      "source": [
        "### training loop\n",
        "def trainMergedVEGA_with_Bayes(vae, data, val_data, epochs=100, beta_en = 0.0001, beta_de = 0.0001, learning_rate = 0.0001):\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr = learning_rate, weight_decay = 5e-4)\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        train_loss_e = 0\n",
        "        valid_loss_e = 0\n",
        "        vae.train() #train mode\n",
        "\n",
        "        for x in data:\n",
        "            x = x.to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            kl_encoder = vae.encoder.kl\n",
        "            kl_decoder = vae.decoder.kl_divergence()\n",
        "\n",
        "            loss = ((x - x_hat)**2).sum() + beta_en* kl_encoder + beta_de*kl_decoder\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss_e += loss.to('cpu').detach().numpy()\n",
        "            vae.decoder.sparse_layer[0].constrain_weights()\n",
        "        train_losses.append(train_loss_e/(len(data)*128))\n",
        "\n",
        "        #### Here you should add the validation loop\n",
        "        vae.eval()\n",
        "\n",
        "        for x in val_data:\n",
        "            x = x.to(device)\n",
        "            x_hat = vae(x)\n",
        "            kl_encoder = vae.encoder.kl\n",
        "            kl_decoder = vae.decoder.kl_divergence()\n",
        "            loss = ((x - x_hat)**2).sum() + beta_en* kl_encoder + beta_de*kl_decoder\n",
        "            valid_loss_e += loss.to('cpu').detach().numpy()\n",
        "        valid_losses.append(valid_loss_e/(len(val_data)*128))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"epoch: \", epoch, \" train_loss: \", train_loss_e/(len(data)*128), \"  valid_loss: \", valid_loss_e/(len(val_data)*128))\n",
        "\n",
        "    return vae, train_losses, valid_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KL05dHYMivn"
      },
      "source": [
        "### Run merged models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrfCwdWEMmBB"
      },
      "outputs": [],
      "source": [
        "def run_merged_vega_model(model_type, train_data, valid_data, cond, activator_gmt, inhibitor_gmt, N):\n",
        "    \"\"\"\n",
        "    model_type: \"vega\", \"swa\", \"bayes\"\n",
        "    cond is \"stimulated\" or \"control\"\n",
        "    activator_gmt is \"activator.gmt\" or \"STAT_activator.gmt\"\n",
        "    inhibitor_gmt is \"inhibitor.gmt\" or \"STAT_inhibitor.gmt\"\n",
        "    \"\"\"\n",
        "\n",
        "    # create mask\n",
        "\n",
        "    train_data, mask = create_merged_mask(train_data, activator_gmt, inhibitor_gmt, add_nodes=1)\n",
        "    train_data = train_data[train_data.obs[\"condition\"] == cond]\n",
        "    valid_data = valid_data[valid_data.obs[\"condition\"] == cond]\n",
        "\n",
        "    # set up dataloader\n",
        "    trainX = torch.utils.data.DataLoader(train_data.X.toarray(), batch_size=128, shuffle=True)\n",
        "    validX = torch.utils.data.DataLoader(valid_data.X.toarray(), batch_size=128, shuffle=True)\n",
        "\n",
        "    all_weights = []\n",
        "\n",
        "    match model_type:\n",
        "        case \"vega\":\n",
        "            for _ in range(N):\n",
        "                vega = MergedVEGA(latent_dims= mask.shape[1], input_dims = mask.shape[0], mask = mask.T, dropout = 0.3, z_dropout = 0.3, soft_penalty=0.1).to(device)\n",
        "                vega, train_losses, valid_losses = trainMergedVEGA_with_valid(vega, trainX, validX, epochs = 50, beta = 0.0001)\n",
        "                weight = get_weight(vega)\n",
        "                all_weights.append(weight)\n",
        "                plot_loss_curve(train_losses, valid_losses)\n",
        "\n",
        "        case \"swa\":\n",
        "            for _ in range(N):\n",
        "                vega = MergedVEGA(latent_dims=mask.shape[1],\n",
        "                        input_dims=mask.shape[0],\n",
        "                        mask=mask.T,\n",
        "                        dropout=0.3,\n",
        "                        z_dropout=0.3, soft_penalty=0.1).to(device)\n",
        "\n",
        "                swa_model, train_losses, valid_losses = trainMergedVEGA_with_swa(\n",
        "                    vega, trainX, validX,\n",
        "                    epochs=50, beta=0.0001,\n",
        "                    learning_rate=0.001,\n",
        "                    swa_start=80,      # you can tune this\n",
        "                    swa_lr=0.0005        # and this\n",
        "                )\n",
        "\n",
        "                W_swa = get_weight_swa(swa_model)\n",
        "                all_weights.append(W_swa)\n",
        "                plot_loss_curve(train_losses, valid_losses)\n",
        "\n",
        "        case \"bayes\":\n",
        "            weight_uncertainties = []\n",
        "            for _ in range(N):\n",
        "                vega = MergedBayesVEGA(latent_dims= mask.shape[1], input_dims = mask.shape[0], mask = mask.T, dropout = 0.3, soft_penalty = 0.0001).to(device)\n",
        "                vega, train_losses, valid_losses = trainMergedVEGA_with_Bayes(vega, trainX, validX, epochs = 60, beta_en = 0.0001, beta_de=0.0001)\n",
        "                weight = get_weight_bayes(vega)\n",
        "                uncertainty = get_weight_uncertainties_bayes(vega)\n",
        "                all_weights.append(weight)\n",
        "                weight_uncertainties.append(uncertainty)\n",
        "                plot_loss_curve(train_losses, valid_losses)\n",
        "\n",
        "            weight_uncertainties = np.stack(weight_uncertainties)\n",
        "            mean_weight_uncertainties = weight_uncertainties.mean(axis=0)\n",
        "            mean_weight_uncertainties = pd.DataFrame(mean_weight_uncertainties, index = train_data.var.index.tolist(), columns=train_data.uns['_vega']['gmv_names'])\n",
        "            mean_weight_uncertainties.to_csv(f\"{cond}_merged_{model_type}_weight_uncertainty_mean.csv\", index=True)\n",
        "\n",
        "\n",
        "    stacked = np.stack(all_weights)\n",
        "    mean_weight = stacked.mean(axis=0)\n",
        "    std_weight = stacked.std(axis=0)\n",
        "\n",
        "    mean_weight = pd.DataFrame(mean_weight, index = train_data.var.index.tolist(), columns=train_data.uns['_vega']['gmv_names'])\n",
        "    std_weight = pd.DataFrame(std_weight, index = train_data.var.index.tolist(), columns=train_data.uns['_vega']['gmv_names'])\n",
        "\n",
        "    mean_weight.to_csv(f\"{cond}_merged_{model_type}_weight_mean.csv\", index=True)\n",
        "    std_weight.to_csv(f\"{cond}_merged_{model_type}_weight_std.csv\", index=True)\n",
        "\n",
        "    return vega, mean_weight, std_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PCkQj1kIbNZ"
      },
      "source": [
        "# Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwLFsXqbIdFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## CHANGE these values to generate different plots\n",
        "TF = \"STAT1\"\n",
        "TF_type = \"activator\"\n",
        "model = \"vega\"\n",
        "\n",
        "# 1) Load means (you already have these)\n",
        "mean_weight_stim = pd.read_csv(f\"stimulated_{TF_type}_{model}_weight_mean.csv\", index_col=0)\n",
        "mean_weight_ctrl = pd.read_csv(f\"control_{TF_type}_{model}_weight_mean.csv\", index_col=0)\n",
        "\n",
        "stim = mean_weight_stim[TF].rename(\"stim\")\n",
        "ctrl = mean_weight_ctrl[TF].rename(\"ctrl\")\n",
        "\n",
        "# 2) Load the std‐weight CSVs in exactly the same shape\n",
        "std_weight_stim  = pd.read_csv(f\"stimulated_{TF_type}_{model}_weight_std.csv\", index_col=0)\n",
        "std_weight_ctrl  = pd.read_csv(f\"control_{TF_type}_{model}_weight_std.csv\",  index_col=0)\n",
        "\n",
        "std_stim = std_weight_stim[TF].rename(\"std_stim\")\n",
        "std_ctrl = std_weight_ctrl[TF].rename(\"std_ctrl\")\n",
        "\n",
        "# 3) Merge everything on the gene index, dropping any missing\n",
        "merged = pd.concat([ctrl, stim, std_ctrl, std_stim], axis=1).dropna()\n",
        "\n",
        "# 4) Compute a single uncertainty score per point\n",
        "#    Here we take the mean of the two std’s, but you could also use max(), sum(), etc.\n",
        "merged[\"uncertainty\"] = merged[[\"std_ctrl\",\"std_stim\"]].mean(axis=1)\n",
        "\n",
        "# 5) Scatter with color = uncertainty\n",
        "plt.figure(figsize=(6,6))\n",
        "sc = plt.scatter(merged[\"ctrl\"], merged[\"stim\"],\n",
        "                 c=merged[\"uncertainty\"],\n",
        "                 cmap=\"viridis\",\n",
        "                 s=20,             # marker size\n",
        "                 edgecolor=\"k\",    # thin black outline\n",
        "                 alpha=0.8)\n",
        "\n",
        "# 6) Diagonal & axes\n",
        "plt.plot([merged[\"ctrl\"].min(), merged[\"ctrl\"].max()],\n",
        "         [merged[\"ctrl\"].min(), merged[\"ctrl\"].max()],\n",
        "         color='grey', linestyle='--')\n",
        "plt.xlabel(\"Control mean weight\")\n",
        "plt.ylabel(\"Stimulated mean weight\")\n",
        "plt.title(f\"{TF} weight: control vs. stimulated\")\n",
        "\n",
        "# 7) Colorbar\n",
        "cbar = plt.colorbar(sc, label=\"Mean STD (uncertainty)\")\n",
        "\n",
        "# 8) Optional: annotate outliers as before\n",
        "merged[\"dist\"] = np.abs(merged[\"stim\"] - merged[\"ctrl\"]) / np.sqrt(2)\n",
        "top = merged.nlargest(20, \"dist\")\n",
        "for gene, row in top.iterrows():\n",
        "    plt.annotate(gene, (row[\"ctrl\"], row[\"stim\"]), fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "c4oya_M17UEw",
        "7PCkQj1kIbNZ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
